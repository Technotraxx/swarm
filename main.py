import streamlit as st
from firecrawl import FirecrawlApp
from swarm import Agent
from openai import OpenAI

# Streamlit App Layout
st.set_page_config(
    page_title="Editorial Nachrichten Assistent",
    page_icon="üì∞",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.title("üì∞ Editorial Nachrichten Assistent")
st.markdown("""
Willkommen beim **Editorial Nachrichten Assistenten**! Geben Sie die URL eines Nachrichtenartikels ein, und unser Assistent erstellt f√ºr Sie ein umfassendes Editorial, indem er die Inhalte scrapt, analysiert, faktenpr√ºft, zusammenfasst und bearbeitet.
""")

# Sidebar f√ºr API-Schl√ºssel Eingaben
st.sidebar.header("API-Konfiguration")

firecrawl_api_key = st.sidebar.text_input(
    "Firecrawl API-Schl√ºssel",
    type="password",
    help="Geben Sie hier Ihren Firecrawl API-Schl√ºssel ein."
)

openai_api_key = st.sidebar.text_input(
    "OpenAI API-Schl√ºssel",
    type="password",
    help="Geben Sie hier Ihren OpenAI API-Schl√ºssel ein."
)

# Initialisierung von FirecrawlApp und OpenAI nur wenn API-Schl√ºssel bereitgestellt werden
if firecrawl_api_key and openai_api_key:
    try:
        app = FirecrawlApp(api_key=firecrawl_api_key)
        client = OpenAI(api_key=openai_api_key)
    except Exception as e:
        st.sidebar.error(f"Fehler bei der Initialisierung der APIs: {str(e)}")
        st.stop()
else:
    st.sidebar.warning("Bitte geben Sie sowohl den Firecrawl- als auch den OpenAI-API-Schl√ºssel ein, um fortzufahren.")
    st.stop()

def scrape_website(url):
    """Eine Website mit Firecrawl scrapen."""
    try:
        scrape_response = app.scrape_url(
            url,
            params={'formats': ['markdown']}
        )
        st.write("üîç Scrape-Antwort:", scrape_response)  # Debugging-Anweisung

        # √úberpr√ºfen ob 'markdown' im Antwort enth√§lt
        if isinstance(scrape_response, dict) and 'markdown' in scrape_response:
            return scrape_response['markdown']
        else:
            raise KeyError("Weder der Schl√ºssel 'markdown' noch 'content' sind in der Scrape-Antwort vorhanden.")
    
    except Exception as e:
        st.error(f"Ein Fehler ist beim Scrapen aufgetreten: {str(e)}")
        return None

def generate_completion(role, task, content):
    """Generiere eine Vervollst√§ndigung mit OpenAI."""
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": f"Du bist ein {role}. {task}"},
                {"role": "user", "content": content}
            ]
        )
        completion = response.choices[0].message.content.strip()
        return completion
    except Exception as e:
        st.error(f"Ein Fehler ist bei der Generierung der Vervollst√§ndigung aufgetreten: {str(e)}")
        return None

def analyze_website_content(content):
    """Analysiere den gescrapten Website-Inhalt f√ºr wichtige Erkenntnisse."""
    analysis = generate_completion(
        "Inhaltsanalyst",
        "Analysiere den folgenden Nachrichteninhalt und gib wichtige Erkenntnisse, einschlie√ülich Relevanz, Bedeutung und potenzieller Auswirkungen.",
        content
    )
    if not analysis:
        st.error("Inhalt konnte nicht analysiert werden.")
    return analysis

def fact_check_content(content):
    """Faktenpr√ºfe den bereitgestellten Inhalt."""
    fact_check = generate_completion(
        "Faktenpr√ºfer",
        "√úberpr√ºfe die faktische Genauigkeit des folgenden Nachrichteninhalts. Hebe eventuelle Unstimmigkeiten hervor oder best√§tige die G√ºltigkeit der Informationen.",
        content
    )
    if not fact_check:
        st.error("Inhalt konnte nicht faktengepr√ºft werden.")
    return fact_check

def summarize_content(content):
    """Fasse den Nachrichteninhalt zusammen."""
    summary = generate_completion(
        "Zusammenfasser",
        "Erstelle eine pr√§gnante Zusammenfassung des folgenden Nachrichtenartikels.",
        content
    )
    if not summary:
        st.error("Inhalt konnte nicht zusammengefasst werden.")
    return summary

def generate_editorial(analysis, fact_check, summary):
    """Generiere ein Editorial basierend auf Analyse, Faktenpr√ºfung und Zusammenfassung."""
    if not all([analysis, fact_check, summary]):
        st.error("Es fehlen Komponenten zur Generierung des Editorials.")
        return None
    editorial = generate_completion(
        "Redakteur",
        "Verfasse einen gut strukturierten Editorial-Artikel unter Verwendung der folgenden Analyse, Faktenpr√ºfung und Zusammenfassung.",
        f"Analyse: {analysis}\nFaktenpr√ºfung: {fact_check}\nZusammenfassung: {summary}"
    )
    if not editorial:
        st.error("Editorial konnte nicht generiert werden.")
    return editorial

# Definition der Agenten

def handoff_to_scraper():
    """√úbergebe die URL an den Website Scraper Agent."""
    return website_scraper_agent

def handoff_to_analyzer():
    """√úbergebe den gescrapten Inhalt an den Content Analyzer Agent."""
    return content_analyzer_agent

def handoff_to_fact_checker():
    """√úbergebe den Inhalt an den Faktenpr√ºfer Agent."""
    return fact_checker_agent

def handoff_to_summarizer():
    """√úbergebe den Inhalt an den Zusammenfasser Agent."""
    return summarizer_agent

def handoff_to_editor():
    """√úbergebe die analysierten und zusammengefassten Inhalte an den Redakteur Agent."""
    return editor_agent

user_interface_agent = Agent(
    name="Benutzeroberfl√§chen-Agent",
    instructions=(
        "Du bist ein Benutzeroberfl√§chen-Agent, der die Interaktionen mit dem Benutzer verwaltet. "
        "Beginne damit, nach einer URL einer Nachrichtenwebsite zu fragen, f√ºr die der Benutzer ein Editorial erstellen m√∂chte. "
        "Stelle bei Bedarf Kl√§rungsfragen. Sei klar und pr√§gnant."
    ),
    functions=[handoff_to_scraper],
)

website_scraper_agent = Agent(
    name="Website Scraper Agent",
    instructions="Du bist ein Website Scraper Agent, spezialisiert auf das Extrahieren von Inhalten aus Nachrichtenwebsites.",
    functions=[scrape_website, handoff_to_analyzer],
)

content_analyzer_agent = Agent(
    name="Inhaltsanalyst Agent",
    instructions=(
        "Du bist ein Inhaltsanalyst Agent, der gescrapten Nachrichteninhalt auf wichtige Erkenntnisse und Relevanz untersucht. "
        "Biete eine gr√ºndliche Analyse zur Unterst√ºtzung der Editorial-Erstellung. Sei pr√§gnant."
    ),
    functions=[analyze_website_content, handoff_to_fact_checker],
)

fact_checker_agent = Agent(
    name="Faktenpr√ºfer Agent",
    instructions=(
        "Du bist ein Faktenpr√ºfer Agent, verantwortlich f√ºr die √úberpr√ºfung der Genauigkeit des Nachrichteninhalts. "
        "Identifiziere und hebe eventuelle Unstimmigkeiten hervor oder best√§tige die G√ºltigkeit der Informationen. Sei pr√§zise."
    ),
    functions=[fact_check_content, handoff_to_summarizer],
)

summarizer_agent = Agent(
    name="Zusammenfasser Agent",
    instructions=(
        "Du bist ein Zusammenfasser Agent, beauftragt mit der Verdichtung des Nachrichteninhalts zu einer pr√§gnanten Zusammenfassung. "
        "Stelle sicher, dass die Zusammenfassung alle wesentlichen Punkte erfasst. Sei klar und b√ºndig."
    ),
    functions=[summarize_content, handoff_to_editor],
)

editor_agent = Agent(
    name="Redakteur Agent",
    instructions=(
        "Du bist ein Redakteur Agent, verantwortlich f√ºr das Verfassen eines ausgefeilten Editorial-Artikels. "
        "Nutze die Analyse, Faktenpr√ºfung und Zusammenfassung, um ein koh√§rentes und ansprechendes St√ºck zu erstellen. "
        "Stelle sicher, dass das Editorial gut strukturiert und fehlerfrei ist."
    ),
    functions=[generate_editorial],
)

# Definition des Arbeitsablaufs
def run_agents(url):
    try:
        # Schritt 1: Website Scrapen
        scraped_content = scrape_website(url)
        if not scraped_content:
            st.error("üî¥ Das Scrapen der Website ist fehlgeschlagen. Bitte √ºberpr√ºfen Sie die URL und versuchen Sie es erneut.")
            return None
        st.success("‚úÖ Website erfolgreich gescrapt.")

        # Schritt 2: Inhalt Analysieren
        analysis = analyze_website_content(scraped_content)
        if not analysis:
            st.error("üî¥ Inhalt konnte nicht analysiert werden.")
            return None
        st.info("üìù Inhalt analysiert.")

        # Schritt 3: Faktenpr√ºfen
        fact_check = fact_check_content(scraped_content)
        if not fact_check:
            st.error("üî¥ Inhalt konnte nicht faktengepr√ºft werden.")
            return None
        st.info("üîç Inhalt faktengepr√ºft.")

        # Schritt 4: Zusammenfassen
        summary = summarize_content(scraped_content)
        if not summary:
            st.error("üî¥ Inhalt konnte nicht zusammengefasst werden.")
            return None
        st.info("üìù Inhalt zusammengefasst.")

        # Schritt 5: Editorial Generieren
        editorial = generate_editorial(analysis, fact_check, summary)
        if not editorial:
            st.error("üî¥ Editorial konnte nicht generiert werden.")
            return None
        st.success("üì∞ Editorial erfolgreich erstellt.")

        return {
            "Analyse": analysis,
            "Faktenpr√ºfung": fact_check,
            "Zusammenfassung": summary,
            "Editorial": editorial
        }
    except Exception as e:
        st.error(f"Ein unerwarteter Fehler ist aufgetreten: {str(e)}")
        return None

# Streamlit App Hauptoberfl√§che
with st.form(key='news_form'):
    u
